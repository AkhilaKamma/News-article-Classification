{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wZAK8vB2mC3D"
      },
      "outputs": [],
      "source": [
        "## Note : Referred Online Tools for Understanding TF-IDF generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the packages\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import json\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import *\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brjunEeaEwEI",
        "outputId": "4bb7c32d-9773-4127-a404-557c74c07bcb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading input files\n",
        "news_corpus = pd.read_csv('news-train-1.csv')\n",
        "with open('dictionary.txt', 'r') as file:\n",
        "    dictionary_words = file.read().splitlines()\n",
        "\n",
        "dict_df = pd.DataFrame({'dict_words': dictionary_words})\n",
        "words_dictionary_list = dict_df['dict_words'].tolist()"
      ],
      "metadata": {
        "id": "jHJKwxlCmori"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "-XDDJUPmnm1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "remove_punctuation = dict((ord(char), None) for char in string.punctuation)\n",
        "def get_processed_tokens(text):\n",
        "    lowers = text.lower()\n",
        "    no_punctuation = lowers.translate(remove_punctuation)\n",
        "    tokens = nltk.word_tokenize(no_punctuation)\n",
        "    filtered = [word for word in tokens if not word in stopwords.words('english')]\n",
        "    lemmatize= []\n",
        "    for item in filtered:\n",
        "        lemmatize.append(ps.stem(item))\n",
        "    return lemmatize\n",
        "\n",
        "news_corpus['cleaned_text'] = news_corpus['Text'].apply(lambda x: get_processed_tokens(x))"
      ],
      "metadata": {
        "id": "i2evfnN6m3hi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_words_from_dictionary(words_list):\n",
        "    filtered_words = []\n",
        "    for word in words_list:\n",
        "        if word in words_dictionary_list:\n",
        "             filtered_words.append(word)\n",
        "    return filtered_words\n",
        "news_corpus['cleaned_filtered_text'] = news_corpus['cleaned_text'].apply(lambda x: filter_words_from_dictionary(x))\n",
        "final_corpus = news_corpus['cleaned_filtered_text'].to_list()"
      ],
      "metadata": {
        "id": "rIiZnwCZocs1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ST8CWBBqqgVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF"
      ],
      "metadata": {
        "id": "xu1SKWiitFTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term_freq_matrix = []\n",
        "lexicons = set()\n",
        "for word_list in final_corpus:\n",
        "    lexicons.update(word_list)\n",
        "lexicons = sorted(list(lexicons))\n",
        "word_to_index = {word: index for index, word in enumerate(lexicons)}"
      ],
      "metadata": {
        "id": "RQGVSTvPtHw9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Creating the Tf matrix while maintaining document order\n",
        "\n",
        "for word_list in final_corpus:\n",
        "    vect = [0] * len(lexicons)\n",
        "    for word in word_list:\n",
        "        if word in word_to_index:\n",
        "            vect[word_to_index[word]] += 1\n",
        "    max_tf = max(vect)\n",
        "    tf_vector_normalized = [tf / max_tf if max_tf > 0 else 0 for tf in vect]\n",
        "    term_freq_matrix.append(tf_vector_normalized)"
      ],
      "metadata": {
        "id": "yAJCGMV0tnXO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating IDF vector\n",
        "num_documents = len(final_corpus)\n",
        "df = [0] * len(lexicons)\n",
        "for words_list in final_corpus:\n",
        "    set_of_words = set(words_list)\n",
        "    for word in set_of_words:\n",
        "        if word in word_to_index:\n",
        "            df[word_to_index[word]] += 1\n",
        "idf_vector = [0 if df_value == 0 else np.log(num_documents / df_value) for df_value in df]"
      ],
      "metadata": {
        "id": "rG0XO2iCw6cs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF matrix\n",
        "tfidf_matrix = []\n",
        "for vect in term_freq_matrix:\n",
        "    tfidf_vector = [tf * idf for tf, idf in zip(vect, idf_vector)]\n",
        "    tfidf_matrix.append(tfidf_vector)\n",
        "\n",
        "# # Convert the TF-IDF matrix to a pandas DataFrame\n",
        "columns = lexicons\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix,columns=columns)\n",
        "\n",
        "# Save the TF-IDF matrix to a text file without column names\n",
        "with open('matrix.txt', 'w', encoding='utf-8') as f:\n",
        "    for row in tfidf_matrix:\n",
        "        f.write(','.join(map(str, row)) + '\\n')\n",
        "\n",
        "print(\"Output saved to matrix.txt\")"
      ],
      "metadata": {
        "id": "kaDbEp-wxv3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc429a3-3c3b-4fc7-f7dc-0f1afeb16545"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved to matrix.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zWYYlYGoyLDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSuMEZJsyLGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top 3 Word Occurances"
      ],
      "metadata": {
        "id": "Odbp5gBj0IBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by category\n",
        "\n",
        "category_order = ['sport', 'business', 'politics', 'entertainment', 'tech']\n",
        "\n",
        "category_data = news_corpus.groupby('Category')['cleaned_filtered_text'].sum()\n",
        "\n",
        "word_frequencies = {category: Counter() for category in category_order}\n",
        "\n",
        "for category, text in category_data.items():\n",
        "    word_count = Counter(text)\n",
        "    word_frequencies[category] = word_count\n",
        "\n",
        "# Find the top 3 most frequent words for each category\n",
        "top_words_per_category = {}\n",
        "for category, word_count in word_frequencies.items():\n",
        "    top_words = word_count.most_common(3)\n",
        "    top_words_per_category[category] = {word: count for word, count in top_words}\n",
        "\n",
        "\n",
        "# Output the result to a JSON file\n",
        "with open('frequency.json', 'w') as json_file:\n",
        "    json.dump(top_words_per_category, json_file, indent=4)\n",
        "\n",
        "print(\"Output saved to frequency.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ciRFG5a0Mp2",
        "outputId": "c6071775-63d2-4010-d7d0-0aa19927a6c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved to frequency.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYDFirwO0RSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top3 average highest tf-idf words"
      ],
      "metadata": {
        "id": "H7OgeGbi0jS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df['Category'] = news_corpus['Category']\n",
        "\n",
        "# Group the data by category\n",
        "grouped = tfidf_df.groupby('Category',sort=False)\n",
        "\n",
        "# Calculate the average TF-IDF scores for each word within each category\n",
        "#average_scores_per_category = {}\n",
        "average_scores_per_category = {category: Counter() for category in category_order}\n",
        "\n",
        "for category, group in grouped:\n",
        "    avg_scores = group.drop(columns=['Category']).mean().sort_values(ascending=False)\n",
        "    top_words = avg_scores[:3]\n",
        "    average_scores_per_category[category] = {word: score for word, score in top_words.items()}\n",
        "\n",
        "# Output the result to a JSON file\n",
        "with open('scores.json', 'w') as json_file:\n",
        "    json.dump(average_scores_per_category, json_file, indent=4)\n",
        "\n",
        "print(\"Output saved to scores.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUqMKEvT0uWn",
        "outputId": "f392e37e-dd9e-4f6a-d061-83ec8017d5ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved to scores.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8I5xWYwl3o0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}